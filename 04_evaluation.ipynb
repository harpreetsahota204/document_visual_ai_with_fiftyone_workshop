{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35afd576",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/04_evaluation.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24165bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9b857",
   "metadata": {},
   "source": [
    "Let's install some plugins to help us along the way. Run the following in your terminal:\n",
    "\n",
    "1. `fiftyone plugins download https://github.com/jacobmarks/keyword-search-plugin`\n",
    "\n",
    "2. `fiftyone plugins download https://github.com/harpreetsahota204/caption-viewer`\n",
    "\n",
    "3. `fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard`\n",
    "\n",
    "This plugin is the main one for this notebook: `fiftyone plugins download https://github.com/harpreetsahota204/text_evaluation_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e37b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/text_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b063d",
   "metadata": {},
   "source": [
    "### Load local dataset\n",
    "\n",
    "You can load the dataset we created in the first notebook as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"neurips-2025-vision-papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82fd63",
   "metadata": {},
   "source": [
    "### (Alternatively) Load dataset from Hugging Face Hub\n",
    "\n",
    "If you're picking up in a fresh Colab notebook or didn't go through the first notebook, you can download the [Visual AI at NeurIPS 2025 dataset with the embeddings from the Jina models we used in the previous notebook](https://huggingface.co/datasets/harpreetsahota/visual_ai_at_neurips2025_jina_with_ocr), hosted on Hugging Face.\n",
    "\n",
    "Note that this dataset we are downloading already has the OCR results parsed, so it will save you time from having to run inference on your own. These models do take painfully long to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801723ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"harpreetsahota/visual_ai_at_neurips2025_jina_with_ocr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba15011",
   "metadata": {},
   "source": [
    "## Text Evaluation Metrics\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Start with ANLS**: It's the standard metric for VLM OCR tasks\n",
    "\n",
    "2. **Use Exact Match as a secondary metric**: Provides a strict accuracy baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd7dcf",
   "metadata": {},
   "source": [
    "### Compute ANLS (Average Normalized Levenshtein Similarity)\n",
    "\n",
    "**Average Normalized Levenshtein Similarity** - Primary metric for VLM OCR evaluation It:\n",
    "\n",
    "- Normalizes edit distance by string length\n",
    "- Applies a configurable threshold (typically 0.5)\n",
    "- Returns 1.0 if similarity â‰¥ threshold, otherwise returns the similarity score\n",
    "- Is robust to minor OCR errors\n",
    "\n",
    "**Use case**: Primary evaluation metric for OCR tasks, VLM document understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "anls_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_anls\")\n",
    "\n",
    "result = anls_op(\n",
    "    dataset, \n",
    "    pred_field=\"md_abstract\", \n",
    "    gt_field=\"abstract\", \n",
    "    output_field=\"md_ansl_score\",\n",
    "    threshold=0.5,\n",
    "    case_sensitive=False,\n",
    "    delegate=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd25bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc454e",
   "metadata": {},
   "source": [
    "### Compute Exact Match\n",
    "\n",
    "**Binary exact match accuracy** between prediction and ground truth. \n",
    "\n",
    "- Case-sensitive option\n",
    "- Whitespace stripping option\n",
    "- Returns 1.0 for perfect match, 0.0 otherwise\n",
    "\n",
    "**Use case**: Strict evaluation where partial credit isn't appropriate (e.g., form field extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "em_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_exact_match\")\n",
    "\n",
    "result = em_op(\n",
    "    dataset, \n",
    "    pred_field=\"md_abstract\", \n",
    "    gt_field=\"abstract\",\n",
    "    output_field=\"md_exact_match_score\",\n",
    "    delegate=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7eebe3",
   "metadata": {},
   "source": [
    "### Compute Normalized Similarity\n",
    "\n",
    "**Continuous similarity score** (0.0-1.0) without threshold\n",
    "\n",
    "- No threshold applied\n",
    "- Full range of similarity values\n",
    "- Useful for ranking and analysis\n",
    "\n",
    "**Use case**: Fine-grained analysis, ranking samples by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "sim_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_normalized_similarity\")\n",
    "\n",
    "result = sim_op(\n",
    "    dataset, \n",
    "    pred_field=\"md_abstract\", \n",
    "    gt_field=\"abstract\",\n",
    "    output_field=\"md_norm_sim_score\",\n",
    "    delegate=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef06e0",
   "metadata": {},
   "source": [
    "### Compute CER\n",
    "\n",
    "**Character Error Rate** - Ratio of character-level edits needed to transform prediction into ground truth.\n",
    "\n",
    "- Based on Levenshtein distance at character level\n",
    "- Lower is better (0.0 = perfect)\n",
    "- Case-sensitive by default\n",
    "\n",
    "**Use case**: Detailed character-level error analysis, language-agnostic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f17bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "cer_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_cer\")\n",
    "\n",
    "result = cer_op(\n",
    "    dataset, \n",
    "    pred_field=\"md_abstract\", \n",
    "    gt_field=\"abstract\",\n",
    "    output_field=\"md_cer_score\",\n",
    "    delegate=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30207341",
   "metadata": {},
   "source": [
    "### Compute WER\n",
    "\n",
    "**Word Error Rate** - Ratio of word-level edits needed to transform prediction into ground truth.\n",
    "\n",
    "- Based on Levenshtein distance at word level\n",
    "- Lower is better (0.0 = perfect)\n",
    "- Case-sensitive by default\n",
    "\n",
    "**Use case**: Speech recognition, word-level accuracy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "wer_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_wer\")\n",
    "\n",
    "result = wer_op(\n",
    "    dataset, \n",
    "    pred_field=\"md_abstract\", \n",
    "    gt_field=\"abstract\",\n",
    "    output_field=\"md_wer_score\",\n",
    "    delegate=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece724f9",
   "metadata": {},
   "source": [
    "### Launch the App and build some dashboards\n",
    "\n",
    "You can launch the App as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da152ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6cc61",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Create the following scatter plots:\n",
    "\n",
    "**1. ANLS vs. Character Error Rate (CER)**  \n",
    "   - **Why**: **Most informative**, as ANLS is a threshold-based similarity score (binary-ish: 0 or 1 with some partial credit), while CER is a continuous error metric measuring character-level edits\n",
    "   - **What it tells us**: \n",
    "     - Shows the relationship between high-level OCR quality and character-level error details\n",
    "     - Reveals whether high ANLS scores correlate with low CER (as expected)\n",
    "     - Can identify cases where strings look \"similar enough\" (high ANLS) but have significant character-level problems\n",
    "     - Helps validate metric consistency\n",
    "\n",
    "**2. Word Error Rate (WER) vs. Character Error Rate (CER)** \n",
    "   - **Why**: Both are continuous error metrics at different levels of granularity\n",
    "   - **What it tells us**:\n",
    "     - Shows whether word-level and character-level errors scale together\n",
    "     - Identifies cases where a few character errors affect entire words vs. minor character mistakes\n",
    "     - Reveals if the OCR errors are more systematic (affecting whole words) or scattered (individual characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7d55d",
   "metadata": {},
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "FiftyOne has a nice [evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) that you can use to assess how well a model performs.\n",
    "\n",
    "By default, `evaluate_classifications` will treat your classifications as generic multiclass predictions, and it will evaluate each prediction by directly comparing its label to the associated ground truth prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"arxiv_category_predictions\",\n",
    "    gt_field=\"arxiv_category_mapped\",\n",
    "    eval_key=\"mapped_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"unmapped_arxiv_category_predictions\",\n",
    "    gt_field=\"arxiv_category\",\n",
    "    eval_key=\"unmapped_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd635b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"md_mapped_categories\",\n",
    "    gt_field=\"arxiv_category\",\n",
    "    eval_key=\"md_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01794279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83311e4d",
   "metadata": {},
   "source": [
    "### My assignment to you\n",
    "\n",
    "Compute the various metrics we introduced in this notebook using the OCR outputs from the other models. You can compare two model outputs against one another to see how much they differ!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
