{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea078e0",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/01_loading_document_datasets.ipynb)\n",
    "\n",
    "\n",
    "Let's download the dependencies for this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ffb72",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "\n",
    "\n",
    "Let's start by downloading a folder from my Google Drive account. This folder contains:\n",
    "\n",
    "- A zip file with PDFs of the first page of Visual AI papers at NeurIPS 2025\n",
    "\n",
    "- A json file with metadata\n",
    "\n",
    "Let's go ahead and download the data.\n",
    "\n",
    "⚠️ This will take ~2GB of disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "folder_id = '1WK-cPumZ2FeiKXEcdD0wDnN0MlBDzCq_'\n",
    "\n",
    "gdown.download_folder(id=folder_id, output='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3194f",
   "metadata": {},
   "source": [
    "Now can extract the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616db225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/neurips_vision_papers.zip -d vision_papers_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaf588",
   "metadata": {},
   "source": [
    "We need images to create a [FiftyOne Dataset](https://docs.voxel51.com/user_guide/using_datasets.html). The following code will convert the PDFs we just downloaded to images so we can create [Samples](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) and parse them to a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ddf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pdf_to_image(pdf_path, output_dir, dpi):\n",
    "    \"\"\"Convert PDF to PNG image.\"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get arxiv_id from filename (remove _page1.pdf)\n",
    "    arxiv_id = Path(pdf_path).stem.replace('_page1', '')\n",
    "    output_path = Path(output_dir) / f\"{arxiv_id}.png\"\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if output_path.exists():\n",
    "        return output_path\n",
    "    \n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=dpi)\n",
    "    images[0].save(output_path, 'PNG')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Convert all PDFs to images\n",
    "pdf_dir = Path(\"vision_papers_pdfs/neurips_vision_papers\")\n",
    "pdf_files = list(pdf_dir.glob(\"*_page1.pdf\"))\n",
    "\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    pdf_to_image(pdf_file, output_dir=\"neurips_vision_papers_images\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1083f",
   "metadata": {},
   "source": [
    "Next we will:\n",
    "\n",
    "- Load the metadata from the json file\n",
    "- Create Samples for a FiftyOne Dataset\n",
    "- Add the Samples to the Dataset\n",
    "- Launch the FiftyOne App to explore what we have\n",
    "\n",
    "Notice that we are parsing `arxiv_category` as a [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c96ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vision_papers = []\n",
    "with open(\"data/neurips_2025_vision_papers.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        vision_papers.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(vision_papers)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FiftyOne dataset\n",
    "dataset = fo.Dataset(\n",
    "    \"neurips-2025-vision-papers\", #name the dataset\n",
    "    overwrite=True, #here in case you need to reuse this dataset name\n",
    "    persistent=True #keep it persistent across Python sessions\n",
    "    )\n",
    "\n",
    "image_dir = Path(\"neurips_vision_papers_images\")\n",
    "\n",
    "# Add samples from vision_papers\n",
    "samples = []\n",
    "for paper in vision_papers:\n",
    "    arxiv_id = paper['arxiv_id']\n",
    "    \n",
    "    # Check if image exists\n",
    "    image_path = image_dir / f\"{arxiv_id}.png\"\n",
    "\n",
    "    # Create sample\n",
    "    sample = fo.Sample(filepath=str(image_path))\n",
    "    \n",
    "    # Add metadata field\n",
    "    sample[\"type\"] = paper[\"type\"]\n",
    "    sample[\"name\"] = paper[\"name\"]\n",
    "    sample[\"virtualsite_url\"] = paper[\"virtualsite_url\"]\n",
    "    sample[\"abstract\"] = paper[\"abstract\"]\n",
    "    sample[\"arxiv_id\"] = arxiv_id\n",
    "    sample[\"arxiv_authors\"] = paper[\"arxiv_authors\"]\n",
    "    \n",
    "    # Add classification for arxiv_category\n",
    "    sample[\"arxiv_category\"] = fo.Classification(label=paper[\"arxiv_category\"])\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "# Add samples to dataset\n",
    "dataset.add_samples(samples)\n",
    "dataset.compute_metadata()\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7929",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "You now have a dataset which contains NeurIPS 2025 accepted papers focused on computer vision and related fields, enriched with arXiv metadata and first-page images.\n",
    "\n",
    "It includes papers from multiple vision-related categories including Computer Vision (cs.CV), Multimedia (cs.MM), Image and Video Processing (eess.IV), Graphics (cs.GR), and Robotics (cs.RO).\n",
    "\n",
    "Each entry includes paper metadata, abstracts, author information, and a high-resolution (500 DPI) PNG image of the paper's first page.\n",
    "\n",
    "Let's call the Dataset.\n",
    "\n",
    "When you \"call the dataset\" in FiftyOne—such as by printing it with `print(dataset)`, you get a summary of the dataset's structure and contents.\n",
    "\n",
    "This includes information like the number of samples, available fields, and possibly a preview of the first or last sample.\n",
    "\n",
    "This is a useful way to inspect your dataset after loading or creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94b6b0",
   "metadata": {},
   "source": [
    "And you can call the [first()](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) method of the Dataset to see what the Sample's schema looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b719a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ea679",
   "metadata": {},
   "source": [
    "We can get a sense of the distribution of `arxiv_category` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"arxiv_category.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef43f3",
   "metadata": {},
   "source": [
    "Now, let's [map these category labels](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.map_labels) to something more human readable. We're doing this because, towards the end of this notebook, we'll use visual document retrieval model to perform zero shot classification of the document images.\n",
    "\n",
    "Begin by [cloning the sample field](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.clone_sample_field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.clone_sample_field(\"arxiv_category\", \"arxiv_category_mapped\")\n",
    "\n",
    "mapping = {\n",
    "    \"cs.CV\": \"Computer Vision\",\n",
    "    \"cs.MM\": \"Multimedia\",\n",
    "    \"eess.IV\": \"Image and Video Processing\",\n",
    "    \"cs.GR\": \"Graphics\",\n",
    "    \"cs.RO\": \"Robotics\",\n",
    "}\n",
    "\n",
    "view = dataset.map_labels(\"arxiv_category_mapped\", mapping)\n",
    "view.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2359ce",
   "metadata": {},
   "source": [
    "And we can verify this worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951944b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c4483",
   "metadata": {},
   "source": [
    "You can launch [FiftyOne App](https://docs.voxel51.com/user_guide/app.html) and visualize the entire dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ba805",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03814697",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "You can checkout other datasets that have already been parsed into FiftyOne format and are hosted on the Hugging Face Hub:\n",
    "\n",
    "- [NutriGreen Dataset](https://huggingface.co/datasets/Voxel51/NutriGreen) - a collection of images representing branded packaged food products\n",
    "\n",
    "- [CommonForms](https://huggingface.co/datasets/Voxel51/commonforms_val_subset) (subset of validation set) - contains 10,000 annotated document images with bounding boxes for three types of form fields: text inputs, choice buttons (checkboxes/radio buttons), and signature fields\n",
    "\n",
    "- [Form Understanding in Noisy Scanned Documents](https://huggingface.co/datasets/Voxel51/form_understanding_in_noisy_scanned_documents_plus) - provides ground truth data for extracting structured information from scanned forms, including entity recognition and relationship extraction between form fields and their values.\n",
    "\n",
    "- [Consolidated Receipt Dataset](https://huggingface.co/datasets/Voxel51/consolidated_receipt_dataset) - contains over 11,000 Indonesian receipts collected from shops and restaurants, featuring images with OCR annotations (bounding boxes and text) and multi-level semantic labels for parsing. This FiftyOne implementation provides an accessible interface for exploring the training split with 800 annotated receipt images.\n",
    "\n",
    "- [Scanned Receipts OCR and Information Extraction Dataset](https://huggingface.co/datasets/Voxel51/scanned_receipts) - comprises 1,000 whole scanned receipt images collected from real-world scenarios.\n",
    "\n",
    "\n",
    "- [Document Haystack (subset)](https://huggingface.co/datasets/Voxel51/document-haystack-10pages) - a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. This expands on the \"Needle in a Haystack\" concept by embedding needles — short key-value statements in pure text or as multimodal text+image snippets — within real-world documents. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
