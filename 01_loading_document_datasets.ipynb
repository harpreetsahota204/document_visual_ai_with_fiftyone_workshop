{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea078e0",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/01_loading_document_datasets.ipynb)\n",
    "\n",
    "\n",
    "Let's download the dependencies for this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ffb72",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "\n",
    "\n",
    "Let's start by downloading a folder from my Google Drive account. This folder contains:\n",
    "\n",
    "- A zip file with PDFs of the first page of Visual AI papers at NeurIPS 2025\n",
    "\n",
    "- A json file with metadata\n",
    "\n",
    "Let's go ahead and download the data.\n",
    "\n",
    "⚠️ This will take ~2GB of disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "folder_id = '1WK-cPumZ2FeiKXEcdD0wDnN0MlBDzCq_'\n",
    "\n",
    "gdown.download_folder(id=folder_id, output='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3194f",
   "metadata": {},
   "source": [
    "Now can extract the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616db225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/neurips_vision_papers.zip -d vision_papers_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaf588",
   "metadata": {},
   "source": [
    "We need images to create a [FiftyOne Dataset](https://docs.voxel51.com/user_guide/using_datasets.html). The following code will convert the PDFs we just downloaded to images so we can create [Samples](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) and parse them to a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0ddf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30787ca8c4244c6db34af246e9bc6284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pdf_to_image(pdf_path, output_dir, dpi):\n",
    "    \"\"\"Convert PDF to PNG image.\"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get arxiv_id from filename (remove _page1.pdf)\n",
    "    arxiv_id = Path(pdf_path).stem.replace('_page1', '')\n",
    "    output_path = Path(output_dir) / f\"{arxiv_id}.png\"\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if output_path.exists():\n",
    "        return output_path\n",
    "    \n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=dpi)\n",
    "    images[0].save(output_path, 'PNG')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Convert all PDFs to images\n",
    "pdf_dir = Path(\"vision_papers_pdfs/neurips_vision_papers\")\n",
    "pdf_files = list(pdf_dir.glob(\"*_page1.pdf\"))\n",
    "\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    pdf_to_image(pdf_file, output_dir=\"neurips_vision_papers_images\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1083f",
   "metadata": {},
   "source": [
    "Next we will:\n",
    "\n",
    "- Load the metadata from the json file\n",
    "- Create Samples for a FiftyOne Dataset\n",
    "- Add the Samples to the Dataset\n",
    "- Launch the FiftyOne App to explore what we have\n",
    "\n",
    "Notice that we are parsing `arxiv_category` as a [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c96ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1134 papers\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "vision_papers = []\n",
    "with open(\"data/neurips_2025_vision_papers.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        vision_papers.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(vision_papers)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n",
      " 100% |███████████████| 1134/1134 [229.0ms elapsed, 0s remaining, 5.0K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |███████████████| 1134/1134 [638.7ms elapsed, 0s remaining, 1.8K samples/s]     \n",
      "Created dataset with 1134 samples\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FiftyOne dataset\n",
    "dataset = fo.Dataset(\n",
    "    \"neurips-2025-vision-papers\", #name the dataset\n",
    "    overwrite=True, #here in case you need to reuse this dataset name\n",
    "    persistent=True #keep it persistent across Python sessions\n",
    "    )\n",
    "\n",
    "image_dir = Path(\"neurips_vision_papers_images\")\n",
    "\n",
    "# Add samples from vision_papers\n",
    "samples = []\n",
    "for paper in vision_papers:\n",
    "    arxiv_id = paper['arxiv_id']\n",
    "    \n",
    "    # Check if image exists\n",
    "    image_path = image_dir / f\"{arxiv_id}.png\"\n",
    "\n",
    "    # Create sample\n",
    "    sample = fo.Sample(filepath=str(image_path))\n",
    "    \n",
    "    # Add metadata field\n",
    "    sample[\"type\"] = paper[\"type\"]\n",
    "    sample[\"name\"] = paper[\"name\"]\n",
    "    sample[\"virtualsite_url\"] = paper[\"virtualsite_url\"]\n",
    "    sample[\"abstract\"] = paper[\"abstract\"]\n",
    "    sample[\"arxiv_id\"] = arxiv_id\n",
    "    sample[\"arxiv_authors\"] = paper[\"arxiv_authors\"]\n",
    "    \n",
    "    # Add classification for arxiv_category\n",
    "    sample[\"arxiv_category\"] = fo.Classification(label=paper[\"arxiv_category\"])\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "# Add samples to dataset\n",
    "dataset.add_samples(samples)\n",
    "dataset.compute_metadata()\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7929",
   "metadata": {},
   "source": [
    "You can inspect the Dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3821c50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        neurips-2025-vision-papers\n",
       "Media type:  image\n",
       "Num samples: 1134\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    type:             fiftyone.core.fields.StringField\n",
       "    name:             fiftyone.core.fields.StringField\n",
       "    virtualsite_url:  fiftyone.core.fields.StringField\n",
       "    abstract:         fiftyone.core.fields.StringField\n",
       "    arxiv_id:         fiftyone.core.fields.StringField\n",
       "    arxiv_authors:    fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    arxiv_category:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94b6b0",
   "metadata": {},
   "source": [
    "And you can call the [first()](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) method of the Dataset to see what the Sample's schema looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b719a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '69139a206c4d6c2f8f6d04c4',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/document_visual_ai_with_fiftyone_workshop/neurips_vision_papers_images/2510.11296v2.png',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': 1173920,\n",
       "        'mime_type': 'image/png',\n",
       "        'width': 4250,\n",
       "        'height': 5500,\n",
       "        'num_channels': 3,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 11, 11, 20, 18, 40, 882000),\n",
       "    'last_modified_at': datetime.datetime(2025, 11, 11, 20, 18, 41, 190000),\n",
       "    'type': 'Poster',\n",
       "    'name': '$\\\\Delta \\\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization',\n",
       "    'virtualsite_url': 'https://neurips.cc/virtual/2025/poster/116579',\n",
       "    'abstract': \"Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities—specifically by directly reducing the maximum cosine similarity to a low value—we introduce a novel OOD score, named $\\\\Delta\\\\mathrm{Energy}$. $\\\\Delta\\\\mathrm{Energy}$ significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, $\\\\Delta\\\\mathrm{Energy}$ can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for $\\\\Delta\\\\mathrm{Energy}$ (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10\\\\%–25\\\\% in AUROC.\",\n",
       "    'arxiv_id': '2510.11296v2',\n",
       "    'arxiv_authors': [\n",
       "        'Lin Zhu',\n",
       "        'Yifeng Yang',\n",
       "        'Xinbing Wang',\n",
       "        'Qinying Gu',\n",
       "        'Nanyang Ye',\n",
       "    ],\n",
       "    'arxiv_category': <Classification: {\n",
       "        'id': '69139a206c4d6c2f8f6d0056',\n",
       "        'tags': [],\n",
       "        'label': 'cs.CV',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c4483",
   "metadata": {},
   "source": [
    "You can launch [FiftyOne App](https://docs.voxel51.com/user_guide/app.html) and visualize the entire dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ba805",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
