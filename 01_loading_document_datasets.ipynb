{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea078e0",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/01_loading_document_datasets.ipynb)\n",
    "\n",
    "\n",
    "Let's download the dependencies for this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ffb72",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "\n",
    "\n",
    "Let's start by downloading a folder from my Google Drive account. This folder contains:\n",
    "\n",
    "- A zip file with PDFs of the first page of Visual AI papers at NeurIPS 2025\n",
    "\n",
    "- A json file with metadata\n",
    "\n",
    "Let's go ahead and download the data.\n",
    "\n",
    "⚠️ This will take ~2GB of disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "folder_id = '1WK-cPumZ2FeiKXEcdD0wDnN0MlBDzCq_'\n",
    "\n",
    "gdown.download_folder(id=folder_id, output='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3194f",
   "metadata": {},
   "source": [
    "Now can extract the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616db225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/neurips_vision_papers.zip -d vision_papers_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaf588",
   "metadata": {},
   "source": [
    "We need images to create a [FiftyOne Dataset](https://docs.voxel51.com/user_guide/using_datasets.html). The following code will convert the PDFs we just downloaded to images so we can create [Samples](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) and parse them to a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0ddf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30787ca8c4244c6db34af246e9bc6284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def pdf_to_image(pdf_path, output_dir, dpi):\n",
    "    \"\"\"Convert PDF to PNG image.\"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get arxiv_id from filename (remove _page1.pdf)\n",
    "    arxiv_id = Path(pdf_path).stem.replace('_page1', '')\n",
    "    output_path = Path(output_dir) / f\"{arxiv_id}.png\"\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if output_path.exists():\n",
    "        return output_path\n",
    "    \n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=dpi)\n",
    "    images[0].save(output_path, 'PNG')\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Convert all PDFs to images\n",
    "pdf_dir = Path(\"vision_papers_pdfs/neurips_vision_papers\")\n",
    "pdf_files = list(pdf_dir.glob(\"*_page1.pdf\"))\n",
    "\n",
    "for pdf_file in tqdm(pdf_files):\n",
    "    pdf_to_image(pdf_file, output_dir=\"neurips_vision_papers_images\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1083f",
   "metadata": {},
   "source": [
    "Next we will:\n",
    "\n",
    "- Load the metadata from the json file\n",
    "- Create Samples for a FiftyOne Dataset\n",
    "- Add the Samples to the Dataset\n",
    "- Launch the FiftyOne App to explore what we have\n",
    "\n",
    "Notice that we are parsing `arxiv_category` as a [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c96ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1134 papers\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "vision_papers = []\n",
    "with open(\"data/neurips_2025_vision_papers.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        vision_papers.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(vision_papers)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87e63d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n",
      " 100% |███████████████| 1134/1134 [229.0ms elapsed, 0s remaining, 5.0K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |███████████████| 1134/1134 [638.7ms elapsed, 0s remaining, 1.8K samples/s]     \n",
      "Created dataset with 1134 samples\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FiftyOne dataset\n",
    "dataset = fo.Dataset(\n",
    "    \"neurips-2025-vision-papers\", #name the dataset\n",
    "    overwrite=True, #here in case you need to reuse this dataset name\n",
    "    persistent=True #keep it persistent across Python sessions\n",
    "    )\n",
    "\n",
    "image_dir = Path(\"neurips_vision_papers_images\")\n",
    "\n",
    "# Add samples from vision_papers\n",
    "samples = []\n",
    "for paper in vision_papers:\n",
    "    arxiv_id = paper['arxiv_id']\n",
    "    \n",
    "    # Check if image exists\n",
    "    image_path = image_dir / f\"{arxiv_id}.png\"\n",
    "\n",
    "    # Create sample\n",
    "    sample = fo.Sample(filepath=str(image_path))\n",
    "    \n",
    "    # Add metadata field\n",
    "    sample[\"type\"] = paper[\"type\"]\n",
    "    sample[\"name\"] = paper[\"name\"]\n",
    "    sample[\"virtualsite_url\"] = paper[\"virtualsite_url\"]\n",
    "    sample[\"abstract\"] = paper[\"abstract\"]\n",
    "    sample[\"arxiv_id\"] = arxiv_id\n",
    "    sample[\"arxiv_authors\"] = paper[\"arxiv_authors\"]\n",
    "    \n",
    "    # Add classification for arxiv_category\n",
    "    sample[\"arxiv_category\"] = fo.Classification(label=paper[\"arxiv_category\"])\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "# Add samples to dataset\n",
    "dataset.add_samples(samples)\n",
    "dataset.compute_metadata()\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7929",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "You now have a dataset which contains NeurIPS 2025 accepted papers focused on computer vision and related fields, enriched with arXiv metadata and first-page images.\n",
    "\n",
    "It includes papers from multiple vision-related categories including Computer Vision (cs.CV), Multimedia (cs.MM), Image and Video Processing (eess.IV), Graphics (cs.GR), and Robotics (cs.RO).\n",
    "\n",
    "Each entry includes paper metadata, abstracts, author information, and a high-resolution (500 DPI) PNG image of the paper's first page.\n",
    "\n",
    "Let's call the Dataset.\n",
    "\n",
    "When you \"call the dataset\" in FiftyOne—such as by printing it with `print(dataset)`, you get a summary of the dataset's structure and contents.\n",
    "\n",
    "This includes information like the number of samples, available fields, and possibly a preview of the first or last sample.\n",
    "\n",
    "This is a useful way to inspect your dataset after loading or creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3821c50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        neurips-2025-vision-papers\n",
       "Media type:  image\n",
       "Num samples: 1134\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    type:             fiftyone.core.fields.StringField\n",
       "    name:             fiftyone.core.fields.StringField\n",
       "    virtualsite_url:  fiftyone.core.fields.StringField\n",
       "    abstract:         fiftyone.core.fields.StringField\n",
       "    arxiv_id:         fiftyone.core.fields.StringField\n",
       "    arxiv_authors:    fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    arxiv_category:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94b6b0",
   "metadata": {},
   "source": [
    "And you can call the [first()](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first) method of the Dataset to see what the Sample's schema looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b719a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '69139a206c4d6c2f8f6d04c4',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/Users/harpreetsahota/workspace/document_visual_ai_with_fiftyone_workshop/neurips_vision_papers_images/2510.11296v2.png',\n",
       "    'tags': [],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': 1173920,\n",
       "        'mime_type': 'image/png',\n",
       "        'width': 4250,\n",
       "        'height': 5500,\n",
       "        'num_channels': 3,\n",
       "    }>,\n",
       "    'created_at': datetime.datetime(2025, 11, 11, 20, 18, 40, 882000),\n",
       "    'last_modified_at': datetime.datetime(2025, 11, 11, 20, 18, 41, 190000),\n",
       "    'type': 'Poster',\n",
       "    'name': '$\\\\Delta \\\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization',\n",
       "    'virtualsite_url': 'https://neurips.cc/virtual/2025/poster/116579',\n",
       "    'abstract': \"Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities—specifically by directly reducing the maximum cosine similarity to a low value—we introduce a novel OOD score, named $\\\\Delta\\\\mathrm{Energy}$. $\\\\Delta\\\\mathrm{Energy}$ significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, $\\\\Delta\\\\mathrm{Energy}$ can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for $\\\\Delta\\\\mathrm{Energy}$ (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10\\\\%–25\\\\% in AUROC.\",\n",
       "    'arxiv_id': '2510.11296v2',\n",
       "    'arxiv_authors': [\n",
       "        'Lin Zhu',\n",
       "        'Yifeng Yang',\n",
       "        'Xinbing Wang',\n",
       "        'Qinying Gu',\n",
       "        'Nanyang Ye',\n",
       "    ],\n",
       "    'arxiv_category': <Classification: {\n",
       "        'id': '69139a206c4d6c2f8f6d0056',\n",
       "        'tags': [],\n",
       "        'label': 'cs.CV',\n",
       "        'confidence': None,\n",
       "        'logits': None,\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ea679",
   "metadata": {},
   "source": [
    "We can get a sense of the distribution of `arxiv_category` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"arxiv_category.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef43f3",
   "metadata": {},
   "source": [
    "Now, let's [map these category labels](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.map_labels) to something more human readable. We're doing this because, towards the end of this notebook, we'll use visual document retrieval model to perform zero shot classification of the document images.\n",
    "\n",
    "Begin by [cloning the sample field](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.clone_sample_field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.clone_sample_field(\"arxiv_category\", \"arxiv_category_mapped\")\n",
    "\n",
    "mapping = {\n",
    "    \"cs.CV\": \"Computer Vision\",\n",
    "    \"cs.MM\": \"Multimedia\",\n",
    "    \"eess.IV\": \"Image and Video Processing\",\n",
    "    \"cs.GR\": \"Graphics\",\n",
    "    \"cs.RO\": \"Robotics\",\n",
    "}\n",
    "\n",
    "view = dataset.map_labels(\"arxiv_category_mapped\", mapping)\n",
    "view.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2359ce",
   "metadata": {},
   "source": [
    "And we can verify this worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951944b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"arxiv_category_mapped.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c4483",
   "metadata": {},
   "source": [
    "You can launch [FiftyOne App](https://docs.voxel51.com/user_guide/app.html) and visualize the entire dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ba805",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03814697",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "You can checkout other datasets that have already been parsed into FiftyOne format and are hosted on the Hugging Face Hub:\n",
    "\n",
    "- [NutriGreen Dataset](https://huggingface.co/datasets/Voxel51/NutriGreen) - a collection of images representing branded packaged food products\n",
    "\n",
    "- [CommonForms](https://huggingface.co/datasets/Voxel51/commonforms_val_subset) (subset of validation set) - contains 10,000 annotated document images with bounding boxes for three types of form fields: text inputs, choice buttons (checkboxes/radio buttons), and signature fields\n",
    "\n",
    "- [Form Understanding in Noisy Scanned Documents](https://huggingface.co/datasets/Voxel51/form_understanding_in_noisy_scanned_documents_plus) - provides ground truth data for extracting structured information from scanned forms, including entity recognition and relationship extraction between form fields and their values.\n",
    "\n",
    "- [Consolidated Receipt Dataset](https://huggingface.co/datasets/Voxel51/consolidated_receipt_dataset) - contains over 11,000 Indonesian receipts collected from shops and restaurants, featuring images with OCR annotations (bounding boxes and text) and multi-level semantic labels for parsing. This FiftyOne implementation provides an accessible interface for exploring the training split with 800 annotated receipt images.\n",
    "\n",
    "- [Scanned Receipts OCR and Information Extraction Dataset](https://huggingface.co/datasets/Voxel51/scanned_receipts) - comprises 1,000 whole scanned receipt images collected from real-world scenarios.\n",
    "\n",
    "\n",
    "- [Document Haystack (subset)](https://huggingface.co/datasets/Voxel51/document-haystack-10pages) - a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. This expands on the \"Needle in a Haystack\" concept by embedding needles — short key-value statements in pure text or as multimodal text+image snippets — within real-world documents. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
