{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790d227f",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/03_using_ocr_models.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eec31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone\n",
    "!pip install \"mineru-vl-utils[transformers]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd310de",
   "metadata": {},
   "source": [
    "### Load local dataset\n",
    "\n",
    "You can load the dataset we created in the first notebook as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"neurips-2025-vision-papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea450d0",
   "metadata": {},
   "source": [
    "### (Alternatively) Load dataset from Hugging Face Hub\n",
    "\n",
    "If you're picking up in a fresh Colab notebook or didn't go through the first notebook, you can download the [Visual AI at NeurIPS 2025 dataset with the embeddings from the Jina models we used in the previous notebook](https://huggingface.co/datasets/harpreetsahota/visual_ai_at_neurips2025_jina), hosted on Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e365c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"harpreetsahota/visual_ai_at_neurips2025_jina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2ecbc",
   "metadata": {},
   "source": [
    " ### Setup the model\n",
    "\n",
    "We'll start by using [MinerU2.5](https://github.com/harpreetsahota204/mineru_2_5), a 1.2B-parameter vision-language model for high-resolution document parsing.\n",
    "\n",
    "This model is good at:\n",
    "\n",
    "- Comprehensive layout analysis (headers, footers, lists, code blocks)\n",
    "\n",
    "- Complex mathematical formula parsing (including mixed Chinese-English)\n",
    "\n",
    "- Robust table parsing (handles rotated, borderless, and partial-border tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed04d2",
   "metadata": {},
   "source": [
    "#### Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8652364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/mineru_2_5\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb89b9",
   "metadata": {},
   "source": [
    "#### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fde8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with default setting\n",
    "model = foz.load_zoo_model(\"opendatalab/MinerU2.5-2509-1.2B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a836b5",
   "metadata": {},
   "source": [
    "#### Use the model for OCR with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Apply model for structured extraction\n",
    "model.operation = \"ocr_detection\"\n",
    "dataset.apply_model(model, label_field=\"text_detections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a0fa2",
   "metadata": {},
   "source": [
    "#### Use the model for OCR text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation = \"ocr\"\n",
    "dataset.apply_model(model, label_field=\"text_extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21666d3e",
   "metadata": {},
   "source": [
    "You can inspect the output as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['text_detections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cca7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['text_extraction']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb02474",
   "metadata": {},
   "source": [
    "### Some models are promptable\n",
    "\n",
    "Below is an example of another VLM for OCR that you can prompt to extract specific content from a document image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c66924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/nanonets_ocr2\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "nanonets_model = foz.load_zoo_model(\"nanonets/Nanonets-OCR2-3B\")\n",
    "\n",
    "nanonets_model.custom_prompt = \"Extract the text from the abstract section of this paper\"\n",
    "\n",
    "# Apply OCR to your dataset\n",
    "dataset.apply_model(nanonets_model, label_field=\"nanonets_abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73aa9a",
   "metadata": {},
   "source": [
    "##### ðŸ“Œ Some other models you may want to check out later:\n",
    "\n",
    "| Model | Parameters | Output | Key Features | Good For |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **[`mineru-2.5`](https://github.com/harpreetsahota204/mineru_2_5)** | 1.2B | Structured markdown | Two-stage strategy: global layout on downsampled image, then fine-grained recognition on native resolution; handles complex math formulas and tables (rotated, borderless, partial-border) | Documents with complex layouts and mathematical content |\n",
    "| **[`deepseek-ocr`](https://docs.voxel51.com/plugins/plugins_ecosystem/deepseek_ocr.html)** | Dual-encoder (SAM + CLIP) | Structured markdown with bounding boxes | Five resolution modes (gundam default uses multi-view processing); contextual optical compression; supports custom prompts for specific extraction tasks | Complex PDFs and multi-column layouts where you need structured output |\n",
    "| **[`olmocr-2`](https://docs.voxel51.com/plugins/plugins_ecosystem/olmocr_2.html)** | 7B (qwen2.5-vl) | Markdown with YAML front matter | Converts equations to LaTeX, tables to HTML; outputs metadata (language, rotation, table/diagram detection); reads documents like a human would | Academic papers and technical documents with equations and structured data |\n",
    "| **[`kosmos-2.5`](https://docs.voxel51.com/plugins/plugins_ecosystem/kosmos2_5.html)** | 1.37B | OCR with bounding boxes or markdown | Two modes (OCR/markdown); automatic hardware optimization (bfloat16/float16/float32); handles handwritten text and diverse document types | General-purpose OCR when you need either coordinates or clean markdown |\n",
    "| **[`nanonets-ocr2`](https://docs.voxel51.com/plugins/plugins_ecosystem/nanonets_ocr2.html)** | 3B | Structured markdown with semantic tags | LaTeX equations, image descriptions, signature/watermark detection, checkboxes to Unicode, tables to HTML, flowcharts as Mermaid, multilingual, VQA support | Documents needing semantic markup and intelligent content recognition |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
