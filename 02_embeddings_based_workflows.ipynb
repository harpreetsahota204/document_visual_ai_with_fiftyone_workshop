{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8d008ee",
      "metadata": {
        "id": "f8d008ee"
      },
      "source": [
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/document_visual_ai_with_fiftyone_workshop/blob/main/02_embeddings_based_workflows.ipynb)\n",
        "\n",
        "### The Challenge:\n",
        "\n",
        "- 1,134 vision papers at NeurIPS\n",
        "- 3 days to explore\n",
        "- Which 30-40 papers should you prioritize?\n",
        "\n",
        "\n",
        "### The Workflow:\n",
        "\n",
        "#### Step 1: Visualize the Landscape\n",
        "\n",
        "- Load dataset ‚Üí Compute embeddings ‚Üí Generate UMAP\n",
        "\n",
        "- See what research clusters emerge: diffusion, transformers, 3D, video\n",
        "\n",
        "- Understand: What's hot? What's emerging? Where do areas overlap?\n",
        "\n",
        "#### Step 2: Find Core Interests\n",
        "\n",
        "- Semantic seach based on your interests\n",
        "\n",
        "- Lasso entire clusters: Tag interesting papers as 'core_interest'\n",
        "\n",
        "- Filter by presentation type: Oral vs Poster\n",
        "\n",
        "#### Step 3: Discover Through Semantic Similarity\n",
        "\n",
        "- Find papers with similar research niches\n",
        "\n",
        "- Find papers similar to ones you already like\n",
        "\n",
        "- Discover cross-domain connections\n",
        "\n",
        "#### Step 4: Identify Novel Work\n",
        "\n",
        "- Sort by representativeness (low scores = outliers)\n",
        "\n",
        "- Papers that don't fit existing categories\n",
        "\n",
        "- Potential breakthroughs or ambitious cross-domain work\n",
        "\n",
        "#### Step 5: Build Your Schedule\n",
        "\n",
        "- Core papers + Adjacent + Outliers\n",
        "\n",
        "- Filter to oral presentations ‚Üí 15 must-attend\n",
        "\n",
        "- Export personalized conference guide\n",
        "\n",
        "#### Setup\n",
        "\n",
        "Let's install our dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47de4614",
      "metadata": {
        "id": "47de4614"
      },
      "outputs": [],
      "source": [
        "!pip install fiftyone torch transformers pillow umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcaea68c",
      "metadata": {
        "id": "dcaea68c"
      },
      "source": [
        "Let's install some plugins to help us along the way. Run the following in your terminal:\n",
        "\n",
        "1. `fiftyone plugins download https://github.com/jacobmarks/keyword-search-plugin`\n",
        "\n",
        "2. `fiftyone plugins download https://github.com/harpreetsahota204/caption-viewer`\n",
        "\n",
        "3. `fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17e1bc1",
      "metadata": {},
      "source": [
        "You can load the dataset we created in the first notebook as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a0bab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "dataset = fo.load_dataset(\"neurips-2025-vision-papers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe3327a",
      "metadata": {
        "id": "cbe3327a"
      },
      "source": [
        "Alternatively, if you're picking up in a fresh Colab notebook or didn't go through the first notebook, you can download the [Visual AI at NeurIPS 2025 dataset](https://huggingface.co/datasets/Voxel51/visual_ai_at_neurips2025), hosted on Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5311d056",
      "metadata": {
        "id": "5311d056"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\"Voxel51/visual_ai_at_neurips2025\", overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4ea4f8",
      "metadata": {
        "id": "5a4ea4f8"
      },
      "source": [
        "### Setup the model\n",
        "\n",
        "For this demo, we're going to use [jina-embeddings-v4](https://docs.voxel51.com/plugins/plugins_ecosystem/jina_embeddings_v4.html).\n",
        "\n",
        "Jina Embeddings v4 is a state-of-the-art Vision Language Model that generates embeddings for both images and text in a shared vector space. It supports multiple tasks including document retrieval, multilingual text matching, and code understanding.\n",
        "\n",
        "##### üìå Some other models you may want to check out later:\n",
        "\n",
        "| Model | Parameters | Output | Key Features | Good For |\n",
        "|:---|:---|:---|:---|:---|\n",
        "| **[`nomic-embed-multimodal`]((https://docs.voxel51.com/plugins/plugins_ecosystem/nomic_embed_multimodal.html))** | 3B and 7B | Multi-dimensional vectors | Available in two sizes | Multimodal embedding tasks|\n",
        "| **[`bimodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/bimodernvbert.html)** | 250M | 768-dim single vectors | Runs fast on CPU - about 7x faster than comparable models | When you need speed and don't have a GPU |\n",
        "| **[`colmodernvbert`](https://docs.voxel51.com/plugins/plugins_ecosystem/colmodernvbert.html)** | 250M | Multi-vectors (ColBERT-style) | Same base as bimodernvbert, matches models 10x its size on vidore benchmarks | Fine-grained document matching with maxsim scoring|\n",
        "| **[`jina-embeddings-v4`](https://docs.voxel51.com/plugins/plugins_ecosystem/jina_embeddings_v4.html)** | 3.8B | 2048-dim single-vector or multi-vector | Supports 30+ languages, task-specific LoRA adapters for retrieval, text-matching, and code | Multilingual document retrieval across different tasks|\n",
        "| **[`colqwen2-5-v0-2`](https://docs.voxel51.com/plugins/plugins_ecosystem/colqwen2_5_v0_2.html)** | qwen2.5-vl-3B | Multi-vectors | Preserves aspect ratios, dynamic resolution up to 768 patches, token pooling keeps ~97.8% accuracy | Document layouts where aspect ratio matters |\n",
        "| **[`colpali-v1-3`](https://docs.voxel51.com/plugins/plugins_ecosystem/colpali_v1_3.html)** | paligemma-3B | Multi-vector late interaction | Original model that showed visual doc retrieval could beat OCR pipelines | Baseline multi-vector retrieval, well-tested |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95aadd3d",
      "metadata": {
        "id": "95aadd3d"
      },
      "source": [
        "### Register the Zoo Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d9b60f",
      "metadata": {
        "id": "f0d9b60f"
      },
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Register this repository as a remote zoo model source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/jina_embeddings_v4\",\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8c32f3",
      "metadata": {
        "id": "af8c32f3"
      },
      "source": [
        "### Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc4c7eb",
      "metadata": {
        "id": "ffc4c7eb"
      },
      "outputs": [],
      "source": [
        "# Load jina model\n",
        "model = foz.load_zoo_model(\n",
        "    \"jinaai/jina-embeddings-v4\",\n",
        "    task=\"retrieval\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a989337",
      "metadata": {
        "id": "4a989337"
      },
      "source": [
        "### Compute embeddings\n",
        "\n",
        "Now, we can use the [`compute_embeddings`](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) method on our entire document collection.\n",
        "\n",
        "This is a one-time operation that turns each document into a vector representation that captures its visual and semantic meaning.\n",
        "\n",
        "**Note:** This took ~1.5 hours on my Mac M3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd41436",
      "metadata": {
        "id": "efd41436"
      },
      "outputs": [],
      "source": [
        "dataset.compute_embeddings(\n",
        "    model=model,\n",
        "    embeddings_field=\"jina_embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220bf7a2",
      "metadata": {
        "id": "220bf7a2"
      },
      "outputs": [],
      "source": [
        "# Check embedding dimensions\n",
        "print(dataset.first()['jina_embeddings'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fe230a",
      "metadata": {
        "id": "d7fe230a"
      },
      "source": [
        "#### ‚ÑπÔ∏è Let me save you sometime\n",
        "\n",
        "If you want to skip waiting for the model run, you can download a dataset with these embeddings (and the zero-shot classifications we do later) and follow along with the rest of the notebook.\n",
        "\n",
        "This is how you can download it:\n",
        "\n",
        "```python\n",
        "import fiftyone as fo\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\"harpreetsahota/visual_ai_at_neurips2025_jina\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7207631f",
      "metadata": {
        "id": "7207631f"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "Once we have embeddings, we can visualize them. This is where magic happens.\n",
        "\n",
        "The [`compute_visualization`](https://docs.voxel51.com/api/fiftyone.brain.visualization.html#fiftyone.brain.visualization.visualize) method in FiftyOne will create a 2D visualization of our document embeddings using UMAP (Uniform Manifold Approximation and Projection).\n",
        "\n",
        "This will help us:\n",
        "\n",
        "- See how documents cluster in the embedding space\n",
        "- Identify similar documents visually\n",
        "- Understand the semantic structure of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbce9c2",
      "metadata": {
        "id": "4bbce9c2"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "results = fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"jina_embeddings\",\n",
        "    method=\"umap\",\n",
        "    brain_key=\"jina_viz\",\n",
        "    num_dims=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92636d67",
      "metadata": {
        "id": "92636d67"
      },
      "source": [
        "When you open the [embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel) in the FiftyOne App, you'll see a bunch of dots.\n",
        "\n",
        "Each dot is a document. Documents that are visually and semantically similar are placed close together.\n",
        "\n",
        "And without us telling it anything about document types or categories, natural clusters emerge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07146f99",
      "metadata": {
        "id": "07146f99"
      },
      "source": [
        "### Build Similarity Index\n",
        "\n",
        "Now let's use the [`compute_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html#fiftyone-brain-similarity) method to build a similarity index. This is where visual document retrieval becomes incredibly powerful for research discovery.\n",
        "\n",
        "This index enables three types of search that transform how you explore 1,134 papers:\n",
        "\n",
        "1. Text-to-image search\n",
        "\n",
        "    Natural language queries like \"diffusion models for medical imaging\" or \"papers with architecture diagrams\" find relevant content in abstracts and visuals.\n",
        "\n",
        "2. Image-to-image search\n",
        "\n",
        "    Click any paper to find others with similar diagrams, notation, or presentation styles.\n",
        "\n",
        "3. Cross-domain discovery\n",
        "    Find connections keywords miss‚Äîlike papers sharing architectural approaches across different fields or citing similar foundational work.\n",
        "\n",
        "Search by semantic meaning, visual structure, and notation style simultaneously. This could help in discovering papers traditional keyword search wouldn't find.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6cc805b",
      "metadata": {
        "id": "d6cc805b"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "text_img_index = fob.compute_similarity(\n",
        "    dataset,\n",
        "    model= \"jinaai/jina-embeddings-v4\",\n",
        "    embeddings_field=\"jina_embeddings\",\n",
        "    brain_key=\"jina_sim\",\n",
        "    model_kwargs={\"task\": \"retrieval\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5889ce13",
      "metadata": {
        "id": "5889ce13"
      },
      "source": [
        "You'll see how to do all this in the App as well, but you can perform semantic similarity search with text queries\n",
        "\n",
        "For this query, we'll retrieve the top 3 most similar documents.\n",
        "\n",
        "[`sort_by_similarity`](https://docs.voxel51.com/api/fiftyone.brain.similarity.html) method returns a `fiftyone.core.view.DatasetView` containing the 3 most similar samples to your text query.\n",
        "\n",
        "You can use this view directly in various ways:\n",
        "\n",
        "- Display it in the FiftyOne App: `session.view = sims`\n",
        "- Iterate over the samples: `for sample in sims: ...`\n",
        "- Apply additional view operations: [`sims.match(...)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match)\n",
        "- Access the samples: `sims.first()`, [`sims.take(n)`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.take), etc.\n",
        "\n",
        "If you want to persist this view for later use, you can [save it to your dataset](https://docs.voxel51.com/user_guide/using_views.html#similarity-views) by tagging the samples or storing the similarity scores in a field using the `dist_field` parameter:\n",
        "\n",
        "This will store the similarity distance for each sample in a field called \"similarity_score\" on the samples themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa1ff20",
      "metadata": {
        "id": "dfa1ff20"
      },
      "outputs": [],
      "source": [
        "sims = text_img_index.sort_by_similarity(\n",
        "    [\"visual document retrieval\"],\n",
        "    k=3,\n",
        "    dist_field=\"similarity_score\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b18f7e4",
      "metadata": {
        "id": "6b18f7e4"
      },
      "source": [
        "### Compute Uniqueness\n",
        "\n",
        "With the embeddings we can [compute a uniqueness score](https://docs.voxel51.com/brain.html#brain-image-uniqueness) for every paper - how different is it from all the others?\n",
        "\n",
        "**`compute_uniqueness`** assigns each paper a uniqueness score (0-1) based on how different it is from the rest of the conference.\n",
        "\n",
        "**Low scores (0.1-0.3)**: Papers in heavily researched areas with incremental variations. Read one representative, skip the rest.\n",
        "\n",
        "**High scores (0.7-0.9)**: Novel approaches that don't fit existing categories. These are your potential breakthrough papers.\n",
        "\n",
        "**Use this to** prioritize unique contributions over the 10th variation of the same idea, and discover papers that don't fit the mainstream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9f7a33",
      "metadata": {
        "id": "6f9f7a33"
      },
      "outputs": [],
      "source": [
        "results = fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    embeddings=\"jina_embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5367599",
      "metadata": {
        "id": "e5367599"
      },
      "source": [
        "### Near Duplicates\n",
        "\n",
        "**[`compute_near_duplicates`](https://docs.voxel51.com/brain.html#near-duplicates)** finds groups of very similar papers by comparing embeddings against a threshold. At a large conference like NeurIPS, this helps you:\n",
        "\n",
        "- **Avoid redundancy**: Don't read multiple papers that are essentially the same approach with minor variations\n",
        "\n",
        "- **Identify research trends**: Find groups of papers from different teams converging on similar solutions\n",
        "\n",
        "- **Efficient scheduling**: If 3 papers in your queue are near-duplicates, attend one talk and skim the others\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73ad8c9",
      "metadata": {
        "id": "e73ad8c9"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "dup_index = fob.compute_near_duplicates(\n",
        "    dataset,\n",
        "    embeddings=\"jina_embeddings\",\n",
        "    threshold=0.051,  # Adjust as needed for your data/model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07334578",
      "metadata": {
        "id": "07334578"
      },
      "source": [
        "This creates two saved views on your dataset:\n",
        "\n",
        "- **`near duplicates`**: All papers that are very similar to one or more other papers. These are your \"related work clusters\" - papers you should compare side-by-side to understand subtle differences in approach.\n",
        "\n",
        "- **`representatives of near duplicates`**: One representative from each cluster of similar papers. Read these first to understand each approach, then decide if the variations are worth diving into.\n",
        "\n",
        "**Example use case**: You find 5 papers about diffusion models for medical imaging that cluster tightly together. Read the representative paper to understand the core approach, then skim the others to see what each team did differently - architecture tweaks, different datasets, alternative loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1164656c",
      "metadata": {
        "id": "1164656c"
      },
      "source": [
        "##### ü§î What's the difference between computing uniqueness and near duplicates?\n",
        "\n",
        "| Method | `compute_near_duplicates` | `compute_uniqueness` |\n",
        "|:---|:---|:---|\n",
        "| **Purpose** | Detects potential near-duplicate samples | Scores how unique each sample is |\n",
        "| **Goal** | Find groups of very similar samples | Rank all samples by uniqueness |\n",
        "| **How it works** | Measures distance between embeddings; samples below threshold are duplicates | Analyzes similarity distribution across entire dataset |\n",
        "| **Output** | `SimilarityIndex` object with duplicate IDs and neighbor mappings | Adds scalar `uniqueness` field (0-1) to each sample |\n",
        "| **Score meaning** | Binary: duplicate or not | Higher = more unique, Lower = more similar to others\n",
        "| **Primary use case** | Dataset cleaning (remove redundant data) | Sample selection (choose diverse samples for annotation/training) |\n",
        "| **Requires threshold** | Yes | No |\n",
        "\n",
        "\n",
        "**Key difference:** One finds duplicates to remove; the other ranks samples to find the most diverse ones to keep.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b32fb45",
      "metadata": {
        "id": "4b32fb45"
      },
      "source": [
        "### Compute Representativeness\n",
        "\n",
        "This finds [the most prototypical](https://docs.voxel51.com/brain.html#image-representativeness) papers in your dataset.\n",
        "\n",
        "##### One way to interpret these scores\n",
        "\n",
        "**High representativeness scores** identify mainstream papers - the ones that best represent each research cluster. These are your \"survey the field\" papers that show what's typical in diffusion models, vision transformers, or 3D reconstruction. If you want to understand the current state of a research area, start here.\n",
        "\n",
        "**Low representativeness scores** identify outliers and boundary papers - the ones that don't fit neatly into existing clusters. These are often the most interesting: novel approaches combining multiple areas, cross-domain applications, or genuinely new methods. These are your \"potential breakthrough\" papers.\n",
        "\n",
        "For conference planning: read the high-representativeness papers to get oriented in each area, then explore the low-representativeness papers to find cutting-edge work that might define future directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4047cc1a",
      "metadata": {
        "id": "4047cc1a"
      },
      "outputs": [],
      "source": [
        "# Compute representativeness scores\n",
        "fob.compute_representativeness(\n",
        "    dataset,\n",
        "    representativeness_field=\"jina_represent\",\n",
        "    method=\"cluster-center\",\n",
        "    embeddings=\"jina_embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3992ab",
      "metadata": {
        "id": "8c3992ab"
      },
      "source": [
        "### Zero-shot Classification\n",
        "\n",
        "We can even use this model to perform zero-shot classification. In this example, we will see how well this model can classify the arXiv category of the paper.\n",
        "\n",
        "Let's get a list of the categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33dcbafa",
      "metadata": {
        "id": "33dcbafa"
      },
      "outputs": [],
      "source": [
        "arxiv_categories = dataset.distinct(\"arxiv_category_mapped.label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bd38c0",
      "metadata": {
        "id": "78bd38c0"
      },
      "outputs": [],
      "source": [
        "arxiv_categories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c779ccbf",
      "metadata": {
        "id": "c779ccbf"
      },
      "source": [
        "Then we can use the [apply_model]() method of the dataset.\n",
        "\n",
        "Notice the `‚Å†text_prompt` argument. This customizes how class names are embedded for comparison with images. It's a template (e.g., \"A research paper from the arXiv category of \") that's combined with each class label to form text inputs like \"A research paper from the arXiv category of Robotics\" or \"A research paper from the arXiv category of Graphics\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd165677",
      "metadata": {
        "id": "bd165677"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "model.text_prompt=\"A research paper from the arXiv category of \"\n",
        "model.classes=arxiv_categories\n",
        "\n",
        "dataset.apply_model(\n",
        "    model,\n",
        "    label_field=\"arxiv_category_predictions\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ca9900",
      "metadata": {
        "id": "45ca9900"
      },
      "source": [
        "We can also see how well it does with unmapped categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50bf5c1a",
      "metadata": {
        "id": "50bf5c1a"
      },
      "outputs": [],
      "source": [
        "unmapped_arxiv_categories = dataset.distinct(\"arxiv_category.label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a3d0e9",
      "metadata": {
        "id": "f9a3d0e9"
      },
      "outputs": [],
      "source": [
        "unmapped_arxiv_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d63508b",
      "metadata": {
        "id": "3d63508b"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "model.text_prompt=\"A research paper from the arXiv category of \"\n",
        "model.classes=unmapped_arxiv_categories\n",
        "\n",
        "dataset.apply_model(\n",
        "    model,\n",
        "    label_field=\"unmapped_arxiv_category_predictions\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5d23e6",
      "metadata": {
        "id": "fc5d23e6"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
